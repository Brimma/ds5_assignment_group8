{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def perform_data_checks(data):\n",
    "    \"\"\"\n",
    "    Voert checks uit op de gegeven data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list): De input data als een list van lists.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Het opgeschoonde DataFrame.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Laad data in DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Verwijderd alle spaties voor/na de inhoud per kolom\n",
    "    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    print(\"Stap 1: Spaties verwijderd\")\n",
    "    \n",
    "    # Controleer op missende waardes\n",
    "    print(\"Stap 2: Controleer op missende waardes\")\n",
    "    if df.isnull().values.any():\n",
    "        print(\"Waarschuwing: De data bevat missende waardes.\")\n",
    "    else:\n",
    "        print(\"Geen missende waardes gevonden\")\n",
    "        \n",
    "    # Controleren op dubbele rijen\n",
    "    print(\"Stap 3: Controleren op dubbele rijen\")\n",
    "    if df.duplicated().any():\n",
    "        print(\"Waarschuwing: De data bevat dubbele rijen.\")\n",
    "    else:\n",
    "        print(\"Geen dubbele rijen gevonden\")\n",
    "        \n",
    "    # Maakt kolommen van type string hoofdletter\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[string_cols] = df[string_cols].apply(lambda x: x.str.upper() if x.dtype == \"object\" else x)\n",
    "    print(\"Stap 4: Strings vervangen door hoofdletters\")\n",
    "    \n",
    "    # Controleert op inconsistente data types per rij\n",
    "    print(\"Stap 5: Controleert op inconsistente data types per rij.\")\n",
    "    inconsistent_columns = []\n",
    "    for col in df.columns:\n",
    "        unique_types = df[col].apply(type).unique()\n",
    "        if len(unique_types) > 1:\n",
    "            inconsistent_columns.append(col)\n",
    "    if inconsistent_columns:\n",
    "        print(f\"Waarschuwing: De data bevat inconsistente data types in de volgende kolommen: {', '.join(inconsistent_columns)}.\")\n",
    "    else:\n",
    "        print(\"Geen inconsistente datatypes gevonden.\")\n",
    "    \n",
    "    # Return het opgeschoonde DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# Voorbeeld\n",
    "file_path = 'dataProject4.xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name='20000-211000')\n",
    "checks= perform_data_checks(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_sales_report(file_path):\n",
    "    \"\"\"\n",
    "    Generates a sales report based on data from an Excel file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the Excel file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The sales report containing total sales and contribution for categories, months, and managers.\n",
    "    \"\"\"\n",
    "    # Excel bestand lezen\n",
    "    data = pd.read_excel(file_path)\n",
    "\n",
    "    # Calculate total sales and contribution by category\n",
    "    category_sales = data.groupby('Category')['Sales'].sum().dropna()\n",
    "    total_sales_category = category_sales.sum()\n",
    "    category_contributions = category_sales / total_sales_category * 100\n",
    "\n",
    "    # Calculate total sales and contribution by month\n",
    "    month_sales = data.groupby('Month')['Sales'].sum()\n",
    "    total_sales_month = month_sales.sum()\n",
    "    month_contributions = month_sales / total_sales_month * 100\n",
    "\n",
    "    # Calculate total sales and contribution by manager\n",
    "    manager_sales = data.groupby('Sales Manager')['Sales'].sum()\n",
    "    total_sales_manager = manager_sales.sum()\n",
    "    manager_contributions = manager_sales / total_sales_manager * 100\n",
    "\n",
    "    # Generate the sales report\n",
    "    report = pd.DataFrame({\n",
    "        'Total Sales by Category': category_sales,\n",
    "        'Contribution Category (%)': category_contributions,\n",
    "        'Total Sales by Month': month_sales,\n",
    "        'Contribution Month (%)': month_contributions,\n",
    "        'Total Sales by Manager': manager_sales,\n",
    "        'Contribution Manager (%)': manager_contributions\n",
    "    })\n",
    "\n",
    "    # Sort the report by category, month, and manager\n",
    "    report = report.sort_values(by=['Total Sales by Category', 'Total Sales by Month', 'Total Sales by Manager'],\n",
    "                                ascending=False)\n",
    "\n",
    "    return report\n",
    "# Specify the file path of the Excel file\n",
    "file_path = 'detailedRetail.xlsx'\n",
    "\n",
    "# Generate the sales report\n",
    "sales_report = generate_sales_report(file_path)\n",
    "\n",
    "# Save the sales report to an Excel file\n",
    "output_file_path = 'reportRetail.xlsx'\n",
    "sales_report.to_excel(output_file_path, index=True)\n",
    "\n",
    "# Confirm the file is saved\n",
    "print(f\"The sales report has been saved to '{output_file_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "# Read file\n",
    "tweets= pd.read_excel('tweets.xlsx')\n",
    "print(tweets)\n",
    "# Create a new column 'language' to store the identified language\n",
    "tweets['language'] = ''\n",
    "\n",
    "# Iterate over each tweet and detect the language\n",
    "for index, row in tweets.iterrows():\n",
    "    text = row['Tweet']\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        tweets.at[index, 'language'] = language\n",
    "    except:\n",
    "        tweets.at[index, 'language'] = 'Unknown'\n",
    "\n",
    "# Print the updated DataFrame with the identified languages\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob\n",
    "pip install nltk\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_excel('tweets.xlsx')\n",
    "\n",
    "# Create a new column 'language' to store the identified language\n",
    "tweets['language'] = ''\n",
    "\n",
    "# Iterate over each tweet and detect the language\n",
    "for index, row in tweets.iterrows():\n",
    "    text = row['Tweet']\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        tweets.at[index, 'language'] = language\n",
    "    except:\n",
    "        tweets.at[index, 'language'] = 'Unknown'\n",
    "\n",
    "# Perform sentiment analysis for English tweets using TextBlob\n",
    "def analyze_sentiment_english(tweet):\n",
    "    blob = TextBlob(tweet)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Perform sentiment analysis for non-English tweets using NLTK's SentimentIntensityAnalyzer\n",
    "def analyze_sentiment_other(tweet):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    scores = sia.polarity_scores(str(tweet))  # Convert tweet to string\n",
    "    compound_score = scores['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply language-specific sentiment analysis to each tweet\n",
    "tweets['sentiment'] = ''\n",
    "for index, row in tweets.iterrows():\n",
    "    tweet = row['Tweet']\n",
    "    language = row['language']\n",
    "    if language == 'en':\n",
    "        sentiment = analyze_sentiment_english(tweet)\n",
    "    else:\n",
    "        sentiment = analyze_sentiment_other(str(tweet))  # Convert tweet to string\n",
    "    tweets.at[index, 'sentiment'] = sentiment\n",
    "\n",
    "# Print the updated DataFrame with the sentiment analysis results\n",
    "print(tweets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
